use std::cmp::{max, min};
use std::path::{Path, PathBuf};
use std::process::exit;
use std::sync::Arc;

use clap::Parser;
use futures::channel::mpsc::Sender;
use futures::future::ready;
use futures::{SinkExt, Stream, StreamExt};
use itertools::Itertools;
use rune::Source;
use status_line::StatusLine;
use tokio::runtime::Builder;
use tokio::task::LocalSet;
use tokio::time::{Duration, Instant};
use tokio_stream::wrappers::IntervalStream;

use config::RunCommand;

use crate::config::{AppConfig, Command, ShowCommand};
use crate::count_down::{BatchedCountDown, CountDown};
use crate::error::{LatteError, Result};
use crate::interrupt::InterruptHandler;
use crate::progress::Progress;
use crate::report::{Report, RunConfigCmp};
use crate::session::*;
use crate::session::{CassError, CassErrorKind, Session, SessionStats};
use crate::stats::{BenchmarkCmp, BenchmarkStats, Recorder};
use crate::workload::{FnRef, Workload, WorkloadStats, LOAD_FN, RUN_FN};

mod config;
mod count_down;
mod error;
mod interrupt;
mod progress;
mod report;
mod session;
mod stats;
mod workload;

#[global_allocator]
static ALLOC: jemallocator::Jemalloc = jemallocator::Jemalloc;

fn interval_stream(rate: f64) -> IntervalStream {
    let interval = Duration::from_nanos(max(1, (1000000000.0 / rate) as u64));
    IntervalStream::new(tokio::time::interval(interval))
}

/// Rounds the duration down to the highest number of whole periods
fn round(duration: Duration, period: Duration) -> Duration {
    let mut duration = duration.as_micros();
    duration /= period.as_micros();
    duration *= period.as_micros();
    Duration::from_micros(duration as u64)
}

/// Fetches session statistics and sends them to the channel
async fn send_stats(workload: &Workload, time: Instant, tx: &mut Sender<Result<WorkloadStats>>) {
    let stats = workload.take_stats(time);
    tx.send(Ok(stats)).await.unwrap();
}

/// Runs a series of requests on a separate thread and
/// produces a stream of QueryStats
fn req_stream(
    count: Arc<CountDown>,
    parallelism: usize,
    rate: Option<f64>,
    sampling_period: Duration,
    workload: Workload,
    progress: Arc<StatusLine<Progress>>,
    interrupt: Arc<InterruptHandler>,
) -> impl Stream<Item=Result<WorkloadStats>> {
    let (mut tx, rx) = futures::channel::mpsc::channel(16);

    std::thread::spawn(move || {
        let rt = Builder::new_current_thread().enable_all().build().unwrap();
        let local = LocalSet::new();
        local.spawn_local(async move {
            // prevent further moves, make it shared for all iterations in this thread
            let workload = &workload;

            let rate = rate.unwrap_or(f64::MAX);
            let mut remaining_count = BatchedCountDown::new(count, 64);

            // A stream of Result<RequestStats> objects, each object generated by an invocation
            // of the action.
            let mut req_stats = {
                interval_stream(rate)
                    .take_while(|_| ready(remaining_count.dec() && !interrupt.is_interrupted()))
                    .enumerate()
                    .map(|(i, _)| workload.run(i as i64))
                    .buffer_unordered(parallelism)
                    .inspect(|_| progress.tick())
            };

            // The following loop takes the stats generated by the stream and aggregates them
            // into chunks of `sampling_period` length. When a chunk is ready, it is sent out.
            let mut start_time = Instant::now();
            workload.reset(start_time);
            while let Some(req) = req_stats.next().await {
                match req {
                    Ok(_) | Err(LatteError::Cassandra(CassError(CassErrorKind::Overloaded(_)))) => {
                        let now = Instant::now();
                        if now - start_time > sampling_period {
                            start_time += round(now - start_time, sampling_period);
                            send_stats(workload, start_time, &mut tx).await;
                        }
                    }

                    // This can happen if an error happened during generation of data or some other
                    // problem on the client-side. In this case there is no point in
                    // continuing.
                    Err(e) => {
                        tx.send(Err(e)).await.unwrap();
                        return;
                    }
                }
            }
            // Send the statistics of remaining requests
            send_stats(workload, Instant::now(), &mut tx).await;
        });
        rt.block_on(local);
    });

    rx
}

/// Waits until one item arrives in each of the streams
/// and collects them into a vector.
/// Finished streams are removed from `streams`.
async fn take_one_of_each<S, T>(streams: &mut Vec<S>) -> Vec<T>
    where
        S: Stream<Item=T> + std::marker::Unpin,
{
    let mut result = Vec::with_capacity(streams.len());
    for i in (0..streams.len()).rev() {
        match streams[i].next().await {
            Some(item) => {
                result.push(item);
            }
            None => {
                streams.swap_remove(i);
            }
        }
    }
    result
}

/// Controls the intensity of requests sent to the server
struct ExecutionOptions {
    /// Maximum rate of requests in requests per second, None means no limit
    rate: Option<f64>,
    /// Number of parallel threads of execution
    threads: usize,
    /// Number of outstanding async requests per each thread
    parallelism: usize,
}

/// Executes the given function many times in parallel.
/// Draws a progress bar.
/// Returns the statistics such as throughput or duration histogram.
///
/// # Parameters
///   - `name`: text displayed next to the progress bar
///   - `count`: number of iterations
///   - `exec_options`: controls execution options such as parallelism level and rate
///   - `workload`: encapsulates a set of queries to execute
async fn par_execute(
    name: &str,
    count: u64,
    exec_options: &ExecutionOptions,
    sampling_period: Duration,
    workload: Workload,
    signals: Arc<InterruptHandler>,
) -> Result<BenchmarkStats> {
    let threads = exec_options.threads;
    let parallelism = exec_options.parallelism;
    let rate = exec_options.rate;

    let progress = Progress::with_count(name.to_string(), count);
    let progress = Arc::new(StatusLine::new(progress));
    let mut stats = Recorder::start(rate, parallelism);

    let mut streams = Vec::with_capacity(threads);
    let count = Arc::new(CountDown::new(count));
    let sub_workloads = workload.split(threads);

    for w in sub_workloads {
        let s = req_stream(
            count.clone(),
            parallelism,
            rate.map(|r| r / (threads as f64)),
            sampling_period,
            w,
            progress.clone(),
            signals.clone(),
        );
        streams.push(s)
    }

    while !streams.is_empty() {
        let samples = take_one_of_each(&mut streams).await;
        let samples: Vec<WorkloadStats> = samples.into_iter().try_collect()?;
        if !samples.is_empty() {
            let aggregate = stats.record(&samples);
            if sampling_period.as_secs() < u64::MAX {
                progress.set_visible(false);
                println!("{}", aggregate);
                progress.set_visible(true);
            }
        }
    }

    Ok(stats.finish())
}

fn load_report_or_abort(path: &Path) -> Report {
    match Report::load(path) {
        Ok(r) => r,
        Err(e) => {
            eprintln!(
                "error: Failed to read report from {}: {}",
                path.display(),
                e
            );
            exit(1)
        }
    }
}

async fn run(conf: RunCommand) -> Result<()> {
    let conf = conf.set_timestamp_if_empty();
    let compare = conf.compare.as_ref().map(|p| load_report_or_abort(p));
    eprintln!(
        "info: Loading workload script {}...",
        conf.workload.display()
    );
    let script = Source::from_path(&conf.workload)
        .map_err(|e| LatteError::ScriptRead(conf.workload.clone(), e))?;

    let mut program = workload::Program::new(script, conf.params.iter().cloned().collect())?;

    if !program.has_run() {
        eprintln!("Function `run` not found in the workload script.")
    }

    eprintln!("info: Connecting to {:?}... ", conf.addresses);
    let mut cluster = cluster(&conf);
    let session = connect_or_abort(&mut cluster).await;

    let mut session = Session::new(session);

    if program.has_schema() {
        eprintln!("info: Creating schema...");
        if let Err(e) = program.schema(&mut session).await {
            eprintln!("error: Failed to create schema: {}", e);
            exit(255);
        }
    }

    if program.has_erase() && !conf.no_load {
        eprintln!("info: Erasing data...");
        if let Err(e) = program.erase(&mut session).await {
            eprintln!("error: Failed to erase: {}", e);
            exit(255);
        }
    }

    if program.has_prepare() {
        eprintln!("info: Preparing...");
        if let Err(e) = program.prepare(&mut session).await {
            eprintln!("error: Failed to prepare: {}", e);
            exit(255);
        }
    }

    let program = Arc::new(program);
    let loader = Workload::new(session.clone(), program.clone(), FnRef::new(LOAD_FN));
    let runner = Workload::new(session.clone(), program.clone(), FnRef::new(RUN_FN));

    let interrupt = Arc::new(InterruptHandler::install());
    let load_options = ExecutionOptions {
        rate: None,
        threads: conf.threads,
        parallelism: min(128, conf.parallelism),
    };

    if !conf.no_load {
        let load_count = program.load_count();
        if load_count > 0 && program.has_load() {
            eprintln!("info: Loading data...");
            par_execute(
                "Loading...",
                load_count,
                &load_options,
                Duration::from_secs(u64::MAX),
                loader,
                interrupt.clone(),
            )
                .await?;
        }
    }

    if interrupt.is_interrupted() {
        return Err(LatteError::Interrupted);
    }

    if conf.warmup_count > 0 {
        eprintln!("info: Warming up...");
        par_execute(
            "Warming up...",
            conf.warmup_count,
            &load_options,
            Duration::from_secs(u64::MAX),
            runner.clone(),
            interrupt.clone(),
        )
            .await?;
    }

    if interrupt.is_interrupted() {
        return Err(LatteError::Interrupted);
    }

    eprintln!("info: Running benchmark...");

    println!(
        "{}",
        RunConfigCmp {
            v1: &conf,
            v2: compare.as_ref().map(|c| &c.conf),
        }
    );

    let exec_options = ExecutionOptions {
        parallelism: conf.parallelism,
        rate: conf.rate,
        threads: conf.threads,
    };
    report::print_log_header();
    let stats = par_execute(
        "Running...",
        conf.count,
        &exec_options,
        Duration::from_secs_f64(conf.sampling_period),
        runner,
        interrupt.clone(),
    )
        .await?;

    let stats_cmp = BenchmarkCmp {
        v1: &stats,
        v2: compare.as_ref().map(|c| &c.result),
    };
    println!();
    println!("{}", &stats_cmp);

    let path = conf
        .output
        .clone()
        .unwrap_or_else(|| PathBuf::from(".latte-report.json"));

    let report = Report::new(conf, stats);
    match report.save(&path) {
        Ok(()) => {}
        Err(e) => {
            eprintln!("error: Failed to save report to {}: {}", path.display(), e);
            exit(1)
        }
    }
    Ok(())
}

async fn show(conf: ShowCommand) -> Result<()> {
    let report1 = load_report_or_abort(&PathBuf::from(conf.report1));
    let report2 = conf
        .report2
        .map(|p| load_report_or_abort(&PathBuf::from(p)));

    let config_cmp = RunConfigCmp {
        v1: &report1.conf,
        v2: report2.as_ref().map(|r| &r.conf),
    };
    println!("{}", config_cmp);

    let results_cmp = BenchmarkCmp {
        v1: &report1.result,
        v2: report2.as_ref().map(|r| &r.result),
    };
    println!("{}", results_cmp);
    Ok(())
}

async fn async_main(command: Command) -> Result<()> {
    match command {
        Command::Run(config) => run(config).await?,
        Command::Show(config) => show(config).await?,
    }
    Ok(())
}

fn main() {
    console::set_colors_enabled(true);
    let command = AppConfig::parse().command;
    let runtime = Builder::new_current_thread().enable_time().build();
    if let Err(e) = runtime.unwrap().block_on(async_main(command)) {
        eprintln!("error: {}", e);
        exit(128);
    }
}

#[cfg(test)]
mod test {
    use super::*;

    #[tokio::test]
    async fn test_take_one_of_each() {
        let s1 = futures::stream::iter(1..=3);
        let s2 = futures::stream::iter(1..=2);
        let s3 = futures::stream::iter(1..=2);
        let mut streams = vec![s1, s2, s3];
        assert_eq!(take_one_of_each(&mut streams).await, vec![1, 1, 1]);
        assert_eq!(take_one_of_each(&mut streams).await, vec![2, 2, 2]);
        assert_eq!(take_one_of_each(&mut streams).await, vec![3]);
        assert_eq!(take_one_of_each(&mut streams).await, Vec::<u32>::new());
        assert!(streams.is_empty());
    }
}
